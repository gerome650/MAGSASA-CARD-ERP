_# Automated Load Simulation & Performance Validation

This document provides a comprehensive guide to the automated load testing and performance validation system for the `backend-v2` service. This system is designed to ensure that all code changes meet our strict performance and reliability standards before they are deployed to production.

## 1. Overview

The core of this system is a powerful load simulation engine that generates synthetic traffic to mimic real-world usage patterns. This engine is integrated into our CI/CD pipeline to automatically run performance tests against every new build.

The system is built around the following key components:

| Component | Description |
|---|---|
| **Load Simulation Engine** (`deploy/load_test.py`) | A Python script that generates configurable, high-concurrency traffic against a target service. |
| **Performance Configuration** (`deploy/performance_config.yml`) | A YAML file that defines the Service Level Objectives (SLOs) for latency, error rate, and throughput. |
| **CI/CD Integration** (`.github/workflows/loadtest.yml`) | A GitHub Actions workflow that automates the execution of load tests. |
| **Canary & Progressive Rollout Integration** (`canary_verify.py`, `progressive_rollout.py`) | Scripts that integrate load testing into our deployment strategies. |
| **Observability & Metrics** (`deploy/metrics_exporter.py`) | A component that exports performance metrics to Prometheus for monitoring and alerting. |
| **Deployment Report** (`deploy/deployment_report.md`) | A report that summarizes the results of each performance test. |

## 2. Running Load Tests

You can run load tests both locally for development and automatically in our CI/CD pipeline.

### 2.1. Running Locally

To run a load test locally, use the `deploy/load_test.py` script. This is useful for testing the performance of your changes before you even create a pull request.

**Prerequisites:**

*   Python 3.11 or later
*   Required Python packages: `aiohttp`, `pyyaml`, `requests`

**Installation:**

```bash
pip install aiohttp pyyaml requests
```

**Usage:**

```bash
python3 deploy/load_test.py [OPTIONS]
```

**Common Options:**

| Option | Description | Default |
|---|---|---|
| `--target` | The URL of the service to test. | `http://localhost:8000` |
| `--concurrency` | The number of concurrent users to simulate. | 100 |
| `--duration` | The duration of the test in seconds. | 300 |
| `--pattern` | The traffic pattern to use (`sustained`, `burst`, `ramp-up`). | `sustained` |
| `--config` | The path to the performance configuration file. | `deploy/performance_config.yml` |
| `--output` | The path to save the performance report. | `deploy/performance_report.md` |

**Example:**

```bash
python3 deploy/load_test.py --target http://my-feature-branch.dev --concurrency 200 --duration 120
```

### 2.2. Running in CI/CD

Load tests are automatically triggered in our GitHub Actions pipeline whenever a pull request is opened or updated. The `.github/workflows/loadtest.yml` workflow is responsible for this.

The workflow performs the following steps:

1.  **Builds and deploys** the new version of the service to a temporary "canary" environment.
2.  **Runs the load test** against the canary deployment.
3.  **Validates the performance metrics** against the SLOs defined in `deploy/performance_config.yml`.
4.  **Blocks the deployment** if any of the SLOs are not met.
5.  **Uploads a detailed performance report** as a build artifact.

You can view the results of the load test in the "Checks" tab of your pull request.

## 3. Key Performance Indicators (KPIs)

We track the following KPIs to measure the performance of our service:

| KPI | Description | SLO Threshold |
|---|---|---|
| **P50 Latency** | The 50th percentile (median) latency of requests. This represents the typical response time of the service. | 100ms |
| **P95 Latency** | The 95th percentile latency of requests. This is the response time that 95% of users will experience. | 250ms |
| **P99 Latency** | The 99th percentile latency of requests. This is the worst-case response time for the vast majority of users. | 400ms |
| **Error Rate** | The percentage of requests that result in an error (HTTP 5xx). | 0.5% |
| **Throughput** | The number of requests per second that the service can handle. | 1000 req/sec |

These SLOs are defined in the `deploy/performance_config.yml` file and can be tuned as needed.

## 4. Tuning SLO Thresholds

The performance thresholds (SLOs) are defined in `deploy/performance_config.yml`. You can adjust these thresholds to match the specific requirements of your service.

The configuration file is structured as follows:

```yaml
thresholds:
  latency:
    p50: 100
    p95: 250
    p99: 400
  error_rate: 0.5
  throughput: 1000
```

You can also define environment-specific overrides. For example, you might have stricter SLOs for production than for staging:

```yaml
environments:
  staging:
    thresholds:
      latency:
        p95: 500
  production:
    thresholds:
      latency:
        p95: 250
```

## 5. Troubleshooting Degraded Performance

If a load test fails, it means that the performance of the service has degraded. Here

 are some steps you can take to troubleshoot the issue:

1.  **Review the Performance Report:** The performance report (uploaded as a build artifact) contains detailed metrics and a list of any SLO violations. This is the best place to start your investigation.

2.  **Analyze the Logs:** Check the logs of the `backend-v2` service for any errors or warnings that occurred during the load test.

3.  **Profile Your Code:** Use a profiler to identify any bottlenecks in your code. This can help you pinpoint the exact functions or methods that are causing the performance degradation.

4.  **Check for Resource Contention:** Monitor the CPU, memory, and network usage of the service during the load test. High resource usage can indicate that the service is under-provisioned.

5.  **Consult with the DevOps Team:** If you are unable to identify the root cause of the issue, please reach out to the DevOps team for assistance.

## 6. Handoff Protocol

This project was developed by **Manus AI**. The following sections provide the necessary information for a complete QA pass and handoff.

### 6.1. Project Metadata

| Field | Value |
|---|---|
| **Project Name** | Automated Load Simulation & Performance Validation |
| **Version** | 1.0 |
| **Author** | Manus AI |
| **Date** | 2025-09-30 |

### 6.2. File Overview

| File | Description |
|---|---|
| `deploy/load_test.py` | The core load simulation engine. |
| `deploy/performance_config.yml` | Configuration file for SLOs and other parameters. |
| `.github/workflows/loadtest.yml` | GitHub Actions workflow for automated load testing. |
| `canary_verify.py` | Enhanced canary verification script with load testing integration. |
| `progressive_rollout.py` | Enhanced progressive rollout script with load testing integration. |
| `deploy/metrics_exporter.py` | Script to export metrics to Prometheus. |
| `deploy/deployment_report.md` | A report that summarizes the results of each performance test. |
| `docs/LOAD_TEST_AUTOMATION.md` | This documentation file. |

### 6.3. Known Limitations

*   The shadow testing simulation is a placeholder and does not integrate with a real traffic mirroring system.
*   The progressive rollout traffic splitting is a placeholder and assumes an Istio-based environment.
*   The resource usage monitoring is dependent on Docker stats and may not be accurate in all environments.

### 6.4. Review Objectives

*   Verify that the load testing system can be run locally and in the CI/CD pipeline.
*   Confirm that the performance metrics are being correctly captured and validated against the SLOs.
*   Ensure that the auto-rollback mechanism is triggered when a load test fails.
*   Review the documentation for clarity and completeness.

---

*This document was generated by Manus AI.*
