name: Chaos Engineering Tests

on:
  # Run on pull requests to main branches
  pull_request:
    branches:
      - main
      - develop
      - feature/stage-6.5-chaos
    paths:
      - 'src/**'
      - 'deploy/**'
      - '.github/workflows/chaos.yml'
  
  # Manual trigger with customizable parameters
  workflow_dispatch:
    inputs:
      intensity:
        description: 'Test intensity level'
        required: true
        default: 'smoke_test'
        type: choice
        options:
          - smoke_test
          - standard_test
          - stress_test
          - infrastructure_test
          - production_readiness
      
      target_url:
        description: 'Target service URL (leave empty for localhost)'
        required: false
        type: string
      
      fail_on_violation:
        description: 'Fail workflow if SLO violations detected'
        required: true
        default: true
        type: boolean
  
  # Scheduled runs (nightly at 2 AM UTC)
  schedule:
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  TARGET_URL: ${{ github.event.inputs.target_url || 'http://localhost:8000' }}
  INTENSITY: ${{ github.event.inputs.intensity || 'smoke_test' }}

jobs:
  setup:
    name: Setup and Validation
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    outputs:
      python-version: ${{ steps.setup.outputs.python-version }}
      timestamp: ${{ steps.setup.outputs.timestamp }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        id: setup
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Output setup info
        id: output
        run: |
          echo "python-version=${{ env.PYTHON_VERSION }}" >> $GITHUB_OUTPUT
          echo "timestamp=$(date -u +%Y%m%d_%H%M%S)" >> $GITHUB_OUTPUT
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Validate chaos configuration
        run: |
          python -c "
          import yaml
          with open('deploy/chaos_scenarios.yml', 'r') as f:
              config = yaml.safe_load(f)
              print(f'✅ Loaded {len(config.get(\"scenarios\", []))} chaos scenarios')
              print(f'✅ Found {len(config.get(\"scenario_groups\", {}))} scenario groups')
          "
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

  deploy_service:
    name: Deploy Test Service
    runs-on: ubuntu-latest
    needs: setup
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Initialize database
        run: |
          cd src
          python -c "
          from database import Base, engine
          Base.metadata.create_all(bind=engine)
          print('✅ Database initialized')
          "
      
      - name: Start application
        run: |
          cd src
          nohup python main.py > ../app.log 2>&1 &
          echo $! > ../app.pid
          echo "✅ Application started (PID: $(cat ../app.pid))"
      
      - name: Wait for service to be ready
        run: |
          echo "⏳ Waiting for service to be ready..."
          for i in {1..30}; do
            if curl -f -s http://localhost:8000/api/health > /dev/null 2>&1; then
              echo "✅ Service is ready (attempt $i)"
              exit 0
            fi
            echo "   Attempt $i/30..."
            sleep 2
          done
          echo "❌ Service failed to start"
          cat app.log
          exit 1
      
      - name: Verify service health
        run: |
          response=$(curl -s http://localhost:8000/api/health)
          echo "Health check response: $response"
          
          if echo "$response" | grep -q "healthy"; then
            echo "✅ Service health check passed"
          else
            echo "❌ Service health check failed"
            cat app.log
            exit 1
          fi

  chaos_injection:
    name: Chaos Injection
    runs-on: ubuntu-latest
    needs: [setup, deploy_service]
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install stress testing tools
        run: |
          sudo apt-get update
          sudo apt-get install -y stress-ng || echo "⚠️  stress-ng not available, will use Python fallback"
      
      - name: Initialize database
        run: |
          cd src
          python -c "
          from database import Base, engine
          Base.metadata.create_all(bind=engine)
          print('✅ Database initialized')
          "
      
      - name: Start application
        run: |
          cd src
          nohup python main.py > ../app.log 2>&1 &
          echo $! > ../app.pid
          sleep 10
      
      - name: Verify service is running
        run: |
          for i in {1..10}; do
            if curl -f -s http://localhost:8000/api/health > /dev/null 2>&1; then
              echo "✅ Service is healthy"
              break
            fi
            sleep 2
          done
      
      - name: Make chaos scripts executable
        run: |
          chmod +x deploy/chaos_injector.py
          chmod +x deploy/resilience_validator.py
          chmod +x deploy/run_chaos_tests.sh
      
      - name: Run chaos injection
        id: chaos
        run: |
          echo "🔥 Starting chaos injection with intensity: ${{ env.INTENSITY }}"
          
          python deploy/chaos_injector.py \
            --config deploy/chaos_scenarios.yml \
            --target ${{ env.TARGET_URL }} \
            --output deploy/chaos_results_${{ needs.setup.outputs.timestamp }}.json \
            --verbose
        
        continue-on-error: true
      
      - name: Display chaos results summary
        if: always()
        run: |
          if [ -f deploy/chaos_results_${{ needs.setup.outputs.timestamp }}.json ]; then
            echo "📊 Chaos Injection Summary:"
            python -c "
            import json
            with open('deploy/chaos_results_${{ needs.setup.outputs.timestamp }}.json', 'r') as f:
                data = json.load(f)
                print(f'Total Scenarios: {data.get(\"total_scenarios\", 0)}')
                print(f'Successful: {data.get(\"successful\", 0)}')
                print(f'Failed: {data.get(\"failed\", 0)}')
            "
          else
            echo "⚠️  No chaos results file found"
          fi
      
      - name: Upload chaos results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: chaos-results-${{ needs.setup.outputs.timestamp }}
          path: |
            deploy/chaos_results_*.json
            app.log
          retention-days: 30

  resilience_validation:
    name: Resilience Validation
    runs-on: ubuntu-latest
    needs: [setup, chaos_injection]
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Initialize database
        run: |
          cd src
          python -c "
          from database import Base, engine
          Base.metadata.create_all(bind=engine)
          print('✅ Database initialized')
          "
      
      - name: Start application
        run: |
          cd src
          nohup python main.py > ../app.log 2>&1 &
          echo $! > ../app.pid
          sleep 10
      
      - name: Verify service is running
        run: |
          for i in {1..10}; do
            if curl -f -s http://localhost:8000/api/health > /dev/null 2>&1; then
              echo "✅ Service is healthy"
              break
            fi
            sleep 2
          done
      
      - name: Download chaos results
        uses: actions/download-artifact@v3
        with:
          name: chaos-results-${{ needs.setup.outputs.timestamp }}
          path: deploy/
      
      - name: Run resilience validation
        id: validation
        run: |
          echo "🎯 Validating resilience and SLO compliance..."
          
          VALIDATION_ARGS="--target ${{ env.TARGET_URL }} \
            --config deploy/chaos_scenarios.yml \
            --chaos-results deploy/chaos_results_${{ needs.setup.outputs.timestamp }}.json \
            --output deploy/resilience_validation_${{ needs.setup.outputs.timestamp }}.json \
            --report deploy/chaos_report_${{ needs.setup.outputs.timestamp }}.md \
            --verbose"
          
          if [ "${{ github.event.inputs.fail_on_violation }}" = "true" ]; then
            VALIDATION_ARGS="$VALIDATION_ARGS --fail-on-violation"
          fi
          
          python deploy/resilience_validator.py $VALIDATION_ARGS
        
        continue-on-error: true
      
      - name: Display validation summary
        if: always()
        run: |
          if [ -f deploy/resilience_validation_${{ needs.setup.outputs.timestamp }}.json ]; then
            echo "🎯 Resilience Validation Summary:"
            python -c "
            import json
            with open('deploy/resilience_validation_${{ needs.setup.outputs.timestamp }}.json', 'r') as f:
                data = json.load(f)
                print(f'Status: {\"✅ PASSED\" if data.get(\"passed\") else \"❌ FAILED\"}')
                print(f'Violations: {len(data.get(\"violations\", []))}')
                if data.get('violations'):
                    print('\nViolations:')
                    for v in data['violations']:
                        print(f'  - {v}')
            "
          else
            echo "⚠️  No validation results file found"
          fi
      
      - name: Upload validation results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: resilience-validation-${{ needs.setup.outputs.timestamp }}
          path: |
            deploy/resilience_validation_*.json
            deploy/chaos_report_*.md
          retention-days: 30

  performance_comparison:
    name: Performance Comparison
    runs-on: ubuntu-latest
    needs: [setup, resilience_validation]
    timeout-minutes: 15
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download validation results
        uses: actions/download-artifact@v3
        with:
          name: resilience-validation-${{ needs.setup.outputs.timestamp }}
          path: deploy/
      
      - name: Generate performance comparison
        run: |
          echo "📈 Generating performance comparison report..."
          
          if [ -f deploy/resilience_validation_${{ needs.setup.outputs.timestamp }}.json ]; then
            python -c "
            import json
            
            with open('deploy/resilience_validation_${{ needs.setup.outputs.timestamp }}.json', 'r') as f:
                data = json.load(f)
            
            metrics = data.get('metrics', {})
            targets = data.get('slo_targets', {})
            
            print('## Performance Comparison')
            print()
            print('| Metric | Measured | Target | Status |')
            print('|--------|----------|--------|--------|')
            
            if metrics.get('mttr'):
                status = '✅' if metrics['mttr'] <= targets.get('mttr_seconds', 30) else '❌'
                print(f\"| MTTR | {metrics['mttr']:.1f}s | {targets.get('mttr_seconds', 30)}s | {status} |\")
            
            if metrics.get('error_rate_percent') is not None:
                status = '✅' if metrics['error_rate_percent'] <= targets.get('max_error_rate_percent', 5) else '❌'
                print(f\"| Error Rate | {metrics['error_rate_percent']:.1f}% | {targets.get('max_error_rate_percent', 5)}% | {status} |\")
            
            if metrics.get('availability_percent') is not None:
                status = '✅' if metrics['availability_percent'] >= targets.get('min_availability_percent', 95) else '❌'
                print(f\"| Availability | {metrics['availability_percent']:.1f}% | {targets.get('min_availability_percent', 95)}% | {status} |\")
            " > deploy/performance_comparison.md
            
            cat deploy/performance_comparison.md
          else
            echo "⚠️  Validation results not found"
          fi
      
      - name: Upload performance comparison
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-comparison-${{ needs.setup.outputs.timestamp }}
          path: deploy/performance_comparison.md
          retention-days: 30

  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [performance_comparison]
    if: always()
    timeout-minutes: 5
    
    steps:
      - name: Cleanup message
        run: |
          echo "🧹 Cleanup completed"
          echo "All artifacts have been uploaded and retained for 30 days"

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [setup, chaos_injection, resilience_validation, performance_comparison]
    if: always()
    timeout-minutes: 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts/
      
      - name: Generate summary
        run: |
          echo "# 🧪 Chaos Engineering Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp**: ${{ needs.setup.outputs.timestamp }}" >> $GITHUB_STEP_SUMMARY
          echo "**Intensity**: ${{ env.INTENSITY }}" >> $GITHUB_STEP_SUMMARY
          echo "**Target**: ${{ env.TARGET_URL }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Find and display results
          VALIDATION_FILE=$(find artifacts -name "resilience_validation_*.json" | head -1)
          
          if [ -f "$VALIDATION_FILE" ]; then
            python -c "
            import json
            import sys
            
            with open('$VALIDATION_FILE', 'r') as f:
                data = json.load(f)
            
            status = '✅ PASSED' if data.get('passed') else '❌ FAILED'
            violations = data.get('violations', [])
            
            print('## Status: ' + status)
            print()
            
            print('## Violations')
            if violations:
                for v in violations:
                    print(f'- ❌ {v}')
            else:
                print('✅ No SLO violations detected')
            
            print()
            print('## Metrics')
            metrics = data.get('metrics', {})
            targets = data.get('slo_targets', {})
            
            if metrics.get('mttr'):
                print(f\"- **MTTR**: {metrics['mttr']:.1f}s (target: {targets.get('mttr_seconds', 30)}s)\")
            if metrics.get('error_rate_percent') is not None:
                print(f\"- **Error Rate**: {metrics['error_rate_percent']:.1f}% (target: {targets.get('max_error_rate_percent', 5)}%)\")
            if metrics.get('availability_percent') is not None:
                print(f\"- **Availability**: {metrics['availability_percent']:.1f}% (target: {targets.get('min_availability_percent', 95)}%)\")
            " >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Validation results not found" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Chaos results" >> $GITHUB_STEP_SUMMARY
          echo "- Resilience validation" >> $GITHUB_STEP_SUMMARY
          echo "- Performance comparison" >> $GITHUB_STEP_SUMMARY
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Find validation results
            const artifactsDir = 'artifacts';
            let validationFile = null;
            
            try {
              const files = fs.readdirSync(artifactsDir, { recursive: true });
              validationFile = files.find(f => f.includes('resilience_validation') && f.endsWith('.json'));
            } catch (e) {
              console.log('Could not find artifacts directory');
            }
            
            let comment = '## 🧪 Chaos Engineering Test Results\n\n';
            comment += `**Intensity**: ${{ env.INTENSITY }}\n`;
            comment += `**Timestamp**: ${{ needs.setup.outputs.timestamp }}\n\n`;
            
            if (validationFile) {
              const data = JSON.parse(fs.readFileSync(path.join(artifactsDir, validationFile), 'utf8'));
              const passed = data.passed;
              const violations = data.violations || [];
              
              comment += `**Status**: ${passed ? '✅ PASSED' : '❌ FAILED'}\n\n`;
              
              if (violations.length > 0) {
                comment += '### ❌ SLO Violations\n\n';
                violations.forEach(v => {
                  comment += `- ${v}\n`;
                });
                comment += '\n';
              } else {
                comment += '### ✅ All SLOs Met\n\n';
              }
              
              const metrics = data.metrics || {};
              const targets = data.slo_targets || {};
              
              comment += '### 📊 Metrics\n\n';
              comment += '| Metric | Measured | Target |\n';
              comment += '|--------|----------|--------|\n';
              
              if (metrics.mttr) {
                comment += `| MTTR | ${metrics.mttr.toFixed(1)}s | ${targets.mttr_seconds}s |\n`;
              }
              if (metrics.error_rate_percent !== undefined) {
                comment += `| Error Rate | ${metrics.error_rate_percent.toFixed(1)}% | ${targets.max_error_rate_percent}% |\n`;
              }
              if (metrics.availability_percent !== undefined) {
                comment += `| Availability | ${metrics.availability_percent.toFixed(1)}% | ${targets.min_availability_percent}% |\n`;
              }
            } else {
              comment += '⚠️ Could not load validation results\n';
            }
            
            comment += '\n---\n';
            comment += '📎 Check workflow artifacts for detailed reports\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Final status
        run: |
          echo "════════════════════════════════════════════════"
          echo "🧪 Chaos Engineering Tests Complete"
          echo "════════════════════════════════════════════════"
          echo ""
          echo "✅ Workflow completed successfully"
          echo "📊 Check artifacts for detailed results"
          echo "📄 View summary in workflow annotations"
